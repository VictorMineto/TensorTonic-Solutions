{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAUjjhOb136"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module9/exercise/module9_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uVgWUZjpb137"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Oa03cAjLb138"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZwAAf2b139"
      },
      "source": [
        "# module9_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQQPZhpb139"
      },
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywk5AxySIWH2"
      },
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "O60bBpZaIWH2"
      },
      "outputs": [],
      "source": [
        "class Agent_0:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        action = self.env.action_space.sample() # your logic here\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PxUUYwdIWH3"
      },
      "source": [
        "### Description\n",
        "\n",
        "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
        "\n",
        "Holes in the ice are distributed in set locations.\n",
        "\n",
        "The player makes moves until they reach the goal or fall in a hole.\n",
        "\n",
        "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
        "\n",
        "The environment is based on :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "dbmsACd0IWH3"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "PTxTBvrkIWH3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, gamma=0.99, theta=1e-10, max_iter=100000):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.theta = theta\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        # FrozenLake: Discrete states/actions\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "\n",
        "        # Transition model (FrozenLake exposes env.unwrapped.P)\n",
        "        self.P = getattr(getattr(env, \"unwrapped\", env), \"P\", None)\n",
        "\n",
        "        # Policy: default random (fallback if P is missing)\n",
        "        self.policy = np.random.randint(self.nA, size=self.nS)\n",
        "\n",
        "        # If we have P, compute optimal policy with value iteration\n",
        "        if self.P is not None:\n",
        "            self.policy = self._compute_optimal_policy_value_iteration()\n",
        "\n",
        "    def _compute_optimal_policy_value_iteration(self):\n",
        "        V = np.zeros(self.nS, dtype=np.float64)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            delta = 0.0\n",
        "            for s in range(self.nS):\n",
        "                # Bellman optimality backup\n",
        "                q_sa = np.zeros(self.nA, dtype=np.float64)\n",
        "                for a in range(self.nA):\n",
        "                    for (p, s2, r, done) in self.P[s][a]:\n",
        "                        # If terminal, no bootstrap after transition\n",
        "                        q_sa[a] += p * (r + (0.0 if done else self.gamma * V[s2]))\n",
        "\n",
        "                v_new = np.max(q_sa)\n",
        "                delta = max(delta, abs(v_new - V[s]))\n",
        "                V[s] = v_new\n",
        "\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "        # Greedy policy w.r.t V\n",
        "        policy = np.zeros(self.nS, dtype=np.int64)\n",
        "        for s in range(self.nS):\n",
        "            q_sa = np.zeros(self.nA, dtype=np.float64)\n",
        "            for a in range(self.nA):\n",
        "                for (p, s2, r, done) in self.P[s][a]:\n",
        "                    q_sa[a] += p * (r + (0.0 if done else self.gamma * V[s2]))\n",
        "\n",
        "\n",
        "            best_actions = np.flatnonzero(q_sa == np.max(q_sa))\n",
        "            policy[s] = int(best_actions[0])  # deterministic\n",
        "\n",
        "        return policy\n",
        "\n",
        "    def _obs_to_state(self, observation):\n",
        "        # Gymnasium can sometimes pass (obs, info)\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]\n",
        "\n",
        "        # If it's a numpy scalar / array([k])\n",
        "        if isinstance(observation, np.ndarray):\n",
        "            if observation.size == 1:\n",
        "                observation = observation.item()\n",
        "            else:\n",
        "                # If one-hot (rare here), use argmax\n",
        "                observation = int(np.argmax(observation))\n",
        "\n",
        "        return int(observation)\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        s = self._obs_to_state(observation)\n",
        "        return int(self.policy[s])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVgcic6eIWH3"
      },
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0UyHFLoIWH3",
        "outputId": "1bd81d09-582c-41c7-ae3a-9a4aa6dca9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
        "agent = Agent(env)\n",
        "\n",
        "observation, _ = env.reset()\n",
        "reward, terminated, truncated, info = None, False, False, None\n",
        "rewards = []\n",
        "while not (terminated or truncated):\n",
        "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "print(f'Cumulative Reward: {sum(rewards)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def eval_agent(env, agent, n_episodes=3000, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    episode_returns = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs, info = env.reset(seed=int(rng.integers(0, 1_000_000)))\n",
        "        terminated = truncated = False\n",
        "        total = 0.0\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            a = agent.choose_action(obs)\n",
        "            obs, r, terminated, truncated, info = env.step(a)\n",
        "            total += r\n",
        "\n",
        "        episode_returns.append(total)\n",
        "\n",
        "    episode_returns = np.array(episode_returns, dtype=float)\n",
        "    mean = episode_returns.mean()\n",
        "    std = episode_returns.std(ddof=1)\n",
        "    # CI 95% approx normale (ok si n grand)\n",
        "    se = np.sqrt(mean * (1 - mean) / n_episodes)  # car 0/1\n",
        "    ci95 = (mean - 1.96 * se, mean + 1.96 * se)\n",
        "    return mean, std, ci95, episode_returns\n",
        "\n",
        "mean, std, ci95, returns = eval_agent(env, agent, n_episodes=5000, seed=42)\n",
        "print(\"Mean reward:\", mean)\n",
        "print(\"Std:\", std)\n",
        "print(\"95% CI:\", ci95)\n",
        "print(\"Meets requirement (>=0.4):\", mean >= 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVNlaM_HL9B4",
        "outputId": "8450c7b5-9316-4c42-907c-13d775a1275f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 0.6254\n",
            "Std: 0.4840678716263539\n",
            "95% CI: (np.float64(0.6119836649911833), np.float64(0.6388163350088166))\n",
            "Meets requirement (>=0.4): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimisation Test 1 : Changer l’objectif : optimiser la probabilité de succès, pas le retour discounté"
      ],
      "metadata": {
        "id": "sg-z7pV9RbRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")"
      ],
      "metadata": {
        "id": "DAfaYsH_Wt-O"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent_1:\n",
        "    def __init__(self, env, gamma=0.99, theta=1e-10, max_iter=100000):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.theta = theta\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        # FrozenLake: Discrete states/actions\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "\n",
        "        # Transition model (FrozenLake exposes env.unwrapped.P)\n",
        "        self.P = getattr(getattr(env, \"unwrapped\", env), \"P\", None)\n",
        "\n",
        "        # Policy: default random (fallback if P is missing)\n",
        "        self.policy = np.random.randint(self.nA, size=self.nS)\n",
        "\n",
        "        # If we have P, compute optimal policy with value iteration\n",
        "        if self.P is not None:\n",
        "            self.policy = self._compute_optimal_policy_reachability()\n",
        "\n",
        "    def _compute_optimal_policy_reachability(self, theta=1e-12, max_iter=100000):\n",
        "        V = np.zeros(self.nS, dtype=np.float64)\n",
        "\n",
        "        # --- trouver goal/holes depuis la grille ---\n",
        "        desc = self.env.unwrapped.desc  # shape (nrow, ncol), dtype 'S1'\n",
        "        nrow, ncol = desc.shape\n",
        "\n",
        "        goal_states = set()\n",
        "        hole_states = set()\n",
        "        for r in range(nrow):\n",
        "            for c in range(ncol):\n",
        "                s = r * ncol + c\n",
        "                if desc[r, c] == b'G':\n",
        "                    goal_states.add(s)\n",
        "                elif desc[r, c] == b'H':\n",
        "                    hole_states.add(s)\n",
        "\n",
        "        terminal_states = goal_states | hole_states\n",
        "\n",
        "        # Fixer les valeurs terminales\n",
        "        for s in goal_states:\n",
        "            V[s] = 1.0\n",
        "        for s in hole_states:\n",
        "            V[s] = 0.0\n",
        "\n",
        "        # --- value iteration \"probabilité d'atteindre le goal\" ---\n",
        "        for _ in range(max_iter):\n",
        "            delta = 0.0\n",
        "            for s in range(self.nS):\n",
        "                if s in terminal_states:\n",
        "                    continue\n",
        "\n",
        "                q_sa = np.zeros(self.nA, dtype=np.float64)\n",
        "                for a in range(self.nA):\n",
        "                    for (p, s2, r, done) in self.P[s][a]:\n",
        "                        # reachability: on propage V(s2) (goal=1, hole=0)\n",
        "                        q_sa[a] += p * V[s2]\n",
        "\n",
        "                v_new = np.max(q_sa)\n",
        "                delta = max(delta, abs(v_new - V[s]))\n",
        "                V[s] = v_new\n",
        "\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # --- politique greedy + tie-break anti-hole (optionnel mais utile) ---\n",
        "        def prob_hole(s, a):\n",
        "            ph = 0.0\n",
        "            for (p, s2, r, done) in self.P[s][a]:\n",
        "                if s2 in hole_states:\n",
        "                    ph += p\n",
        "            return ph\n",
        "\n",
        "        policy = np.zeros(self.nS, dtype=np.int64)\n",
        "        for s in range(self.nS):\n",
        "            if s in terminal_states:\n",
        "                policy[s] = 0\n",
        "                continue\n",
        "\n",
        "            q_sa = np.zeros(self.nA, dtype=np.float64)\n",
        "            for a in range(self.nA):\n",
        "                for (p, s2, r, done) in self.P[s][a]:\n",
        "                    q_sa[a] += p * V[s2]\n",
        "\n",
        "            best = np.flatnonzero(q_sa == np.max(q_sa))\n",
        "            if len(best) > 1:\n",
        "                holes = np.array([prob_hole(s, a) for a in best])\n",
        "                best = best[np.flatnonzero(holes == holes.min())]\n",
        "\n",
        "            policy[s] = int(np.random.choice(best))\n",
        "\n",
        "        return policy\n",
        "\n",
        "    def _obs_to_state(self, observation):\n",
        "        # Gymnasium can sometimes pass (obs, info)\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]\n",
        "\n",
        "        # If it's a numpy scalar / array([k])\n",
        "        if isinstance(observation, np.ndarray):\n",
        "            if observation.size == 1:\n",
        "                observation = observation.item()\n",
        "            else:\n",
        "                # If one-hot (rare here), use argmax\n",
        "                observation = int(np.argmax(observation))\n",
        "\n",
        "        return int(observation)\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        s = self._obs_to_state(observation)\n",
        "        return int(self.policy[s])"
      ],
      "metadata": {
        "id": "Qrw_EgTwNE1l"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent_1(env)\n",
        "mean, std, ci95, returns = eval_agent(env, agent, n_episodes=5000, seed=42)\n",
        "print(\"Mean reward:\", mean)\n",
        "print(\"Std:\", std)\n",
        "print(\"95% CI:\", ci95)\n",
        "print(\"Meets requirement (>=0.4):\", mean >= 0.4)"
      ],
      "metadata": {
        "id": "1Xb5wPVUSUHv",
        "outputId": "d2d84b16-a5a1-4381-bd93-c26343a206c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 0.5168\n",
            "Std: 0.49976765956062874\n",
            "95% CI: (np.float64(0.5029485325916999), np.float64(0.5306514674083002))\n",
            "Meets requirement (>=0.4): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimisation Test 2 : Vérifier le vrai horizon (TimeLimit) et l'optimiser"
      ],
      "metadata": {
        "id": "TvAzalnBXph-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "H = env.spec.max_episode_steps\n",
        "print(\"max_episode_steps =\", H)"
      ],
      "metadata": {
        "id": "BLOQrFLVXlPx",
        "outputId": "474c2619-22e4-4cc4-ccd7-55c168d5bc5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_episode_steps = 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent_2:\n",
        "    def __init__(self, env, theta=1e-10, max_iter=100000):\n",
        "        self.env = env\n",
        "        self.theta = theta\n",
        "        self.max_iter = max_iter\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "        self.P = getattr(getattr(env, \"unwrapped\", env), \"P\", None)\n",
        "\n",
        "        desc = self.env.unwrapped.desc\n",
        "        nrow, ncol = desc.shape\n",
        "\n",
        "        def cell(i, j):\n",
        "            v = desc[i, j]\n",
        "            return v.decode(\"utf-8\") if isinstance(v, (bytes, np.bytes_)) else str(v)\n",
        "\n",
        "        self.start_state = None\n",
        "        self.goal_states = set()\n",
        "        self.hole_states = set()\n",
        "\n",
        "        for r in range(nrow):\n",
        "            for c in range(ncol):\n",
        "                s = r * ncol + c\n",
        "                ch = cell(r, c)\n",
        "                if ch == \"S\":\n",
        "                    self.start_state = s\n",
        "                elif ch == \"G\":\n",
        "                    self.goal_states.add(s)\n",
        "                elif ch == \"H\":\n",
        "                    self.hole_states.add(s)\n",
        "\n",
        "        if self.start_state is None:\n",
        "            self.start_state = 0\n",
        "\n",
        "        self.t = 0\n",
        "        self.H = env.spec.max_episode_steps if env.spec is not None else 100\n",
        "\n",
        "        self.policy_t = None\n",
        "        self.policy = np.random.randint(self.nA, size=self.nS)\n",
        "\n",
        "        if self.P is not None:\n",
        "            self.policy_t = self._compute_optimal_policy_finite_horizon(self.H)\n",
        "            self.policy = self.policy_t[0]\n",
        "\n",
        "\n",
        "    def _compute_optimal_policy_finite_horizon(self, H):\n",
        "        desc = self.env.unwrapped.desc\n",
        "        nrow, ncol = desc.shape\n",
        "\n",
        "        goal_states = set()\n",
        "        hole_states = set()\n",
        "        for r in range(nrow):\n",
        "            for c in range(ncol):\n",
        "                s = r * ncol + c\n",
        "                if desc[r, c] == b'G':\n",
        "                    goal_states.add(s)\n",
        "                elif desc[r, c] == b'H':\n",
        "                    hole_states.add(s)\n",
        "        terminal_states = goal_states | hole_states\n",
        "\n",
        "        V_next = np.zeros(self.nS, dtype=np.float64)\n",
        "        for s in goal_states:\n",
        "            V_next[s] = 1.0\n",
        "\n",
        "        policy_t = np.zeros((H, self.nS), dtype=np.int64)\n",
        "\n",
        "        for t in reversed(range(H)):\n",
        "            V = V_next.copy()\n",
        "\n",
        "            for s in range(self.nS):\n",
        "                if s in terminal_states:\n",
        "                    policy_t[t, s] = 0\n",
        "                    continue\n",
        "\n",
        "                q_sa = np.zeros(self.nA, dtype=np.float64)\n",
        "                for a in range(self.nA):\n",
        "                    for (p, s2, r, done) in self.P[s][a]:\n",
        "                        q_sa[a] += p * V_next[s2]\n",
        "\n",
        "                best = np.flatnonzero(q_sa == np.max(q_sa))\n",
        "                if len(best) > 1:\n",
        "                    def prob_hole(a):\n",
        "                        ph = 0.0\n",
        "                        for (p, s2, r, done) in self.P[s][a]:\n",
        "                            if s2 in hole_states:\n",
        "                                ph += p\n",
        "                        return ph\n",
        "                    holes = np.array([prob_hole(a) for a in best])\n",
        "                    best = best[np.flatnonzero(holes == holes.min())]\n",
        "\n",
        "                policy_t[t, s] = int(best[0])\n",
        "                V[s] = np.max(q_sa)\n",
        "\n",
        "            V_next = V\n",
        "\n",
        "        return policy_t\n",
        "\n",
        "    def _obs_to_state(self, observation):\n",
        "        if isinstance(observation, tuple):\n",
        "            observation = observation[0]\n",
        "\n",
        "        if isinstance(observation, np.ndarray):\n",
        "            if observation.size == 1:\n",
        "                observation = observation.item()\n",
        "            else:\n",
        "                observation = int(np.argmax(observation))\n",
        "\n",
        "        return int(observation)\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        s = self._obs_to_state(observation)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            self.t = 0\n",
        "\n",
        "        if s == self.start_state and self.t > 0:\n",
        "            self.t = 0\n",
        "\n",
        "        if self.policy_t is not None:\n",
        "            tt = self.t if self.t < self.H else self.H - 1\n",
        "            a = int(self.policy_t[tt, s])\n",
        "        else:\n",
        "            a = int(self.policy[s])\n",
        "\n",
        "        self.t += 1\n",
        "        return a"
      ],
      "metadata": {
        "id": "6UzEgYplVsYC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent_2(env)\n",
        "mean, std, ci95, returns = eval_agent(env, agent, n_episodes=5000, seed=42)\n",
        "print(\"Mean reward:\", mean)\n",
        "print(\"Std:\", std)\n",
        "print(\"95% CI:\", ci95)\n",
        "print(\"Meets requirement (>=0.4):\", mean >= 0.4)"
      ],
      "metadata": {
        "id": "USsTTT5gYm65",
        "outputId": "7933dec0-ccfd-4e2c-f916-a08005c942fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 0.6308\n",
            "Std: 0.4826364548907572\n",
            "95% CI: (np.float64(0.6174233378709336), np.float64(0.6441766621290664))\n",
            "Meets requirement (>=0.4): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VjNwqvweYn5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}